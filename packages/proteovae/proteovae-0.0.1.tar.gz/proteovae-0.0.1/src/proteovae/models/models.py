import torch
from torch import nn
import torch.distributions as dist
import torch.nn.functional as F
from .utils import *
from .base import *
from typing import Optional


class BaseVAE(nn.Module):
    r"""Base variational autoencoder model.

    Args:
        model_config (~utils.BaseConfig): config file specifying generic properties of the vae 

        encoder (Encoder): Instance of an encoder-like architecture.  Works with any :class:`~nn.Module` object mapping input tensors to hyperparameters of the posterior over the embeddings. 

        decoder (Decoder): Instance of a decoder-like architecture. Works with any :class:`~nn.Module` object mapping embeddings to reconstructions. 


    .. note::
        Abstract class, not meant to be directly instantiated! Subclass this when building new VAEs or use the :class:`~GuidedVAE` model

    """

    def __init__(self,
                 config: BaseConfig,
                 encoder: Optional[nn.Module] = None,
                 decoder: Optional[nn.Module] = None
                 ):

        super().__init__()
        self.model_config = config
        self.input_dim = self.model_config.input_dim
        self.latent_dim = self.model_config.latent_dim
        self.device = self.model_config.device

        self.encoder = encoder
        self.decoder = decoder

    def forward(self, x):
        raise NotImplementedError

    def loss_function(self, x):
        raise NotImplementedError

    def val_function(self, x):
        raise NotImplementedError


class GuidedVAE(BaseVAE):
    """Guided variational autoencoder implementation.

    Args:
        model_config (~utils.GuidedConfig): config file specifying generic properties of the vae 

        encoder (~base.Encoder): Instance of an encoder-like architecture.  Works with any :class:`~nn.Module` object  mapping input tensors to hyperparameters of the posterior over the embeddings. 

        decoder (~base.Decoder): Instance of a decoder-like architecture. Works with any :class:`~nn.Module` object mapping embeddings to reconstructions. 

        guide (~base.Guide): Logistic regression network to predict labels at latent scale 


    .. note::
        Convention for this model is to allocate the guided dimensions counting in reverse.  For example, in a model with an embedding dimension of size 10 and 2 guided units, we allocate guided = units[-2,-1]

    """

    def __init__(self, guide, **kwargs):
        super().__init__(**kwargs)

        # Tuning ELBO
        self.beta = self.model_config.beta
        self.eta = self.model_config.eta
        self.gamma = self.model_config.gamma

        self.elbo_scheduler = self.model_config.elbo_scheduler

        # Guide
        self.guide = guide
        self.guided_dim = self.model_config.guided_dim

        # Latent prior
        self.pz = dist.normal.Normal(torch.zeros(1, self.latent_dim, device=self.device),
                                     torch.ones(self.latent_dim, device=self.device))

    def forward(self, x):
        r"""Outputs posterior latent distributions and sample embeddings for batch 

        Args:
            x (~torch.Tensor): input batch 

        Returns:
            qz_x, decoded (~torch.distributions, ~torch.Tensor): posterior distribution and reconstruction
        """
        encoded = self.encoder(x)
        mu, log_var = encoded['cont']

        # reparameterization trick
        z = mu + torch.exp(0.5*log_var) * \
            (torch.zeros_like(mu, device=self.device).normal_())
        decoded = self.decoder(z)

        # posterior
        qz_x = dist.normal.Normal(mu, torch.exp(0.5*log_var))

        return qz_x, decoded

    def loss_function(self, data):
        r"""
        Multi-task objective motivating this entire project

        .. math::
             \begin{align*}
            \mathcal{L}(\boldsymbol{\theta}, \boldsymbol{\phi}; x) &= \mathbb{E}_{z \sim q_{\boldsymbol{\phi}}(z|x)}\left[\log p_{\boldsymbol{\theta}}(x|z)\right] - 
                \beta D_{KL}(q_{\boldsymbol{\phi}}(z|x)||p_{\boldsymbol{\theta}}(z)) - \eta H(Y, \psi(\bar{Z}))\\
                & - \left(\gamma_{od}\sum_{i\neq j}||[Cov(z)]_{i,j}|| + \gamma_{d}\sum_{i=1}^{d}||[Cov(z)]_{ii}-1||\right)
            \end{align*} 

        with 

        .. math :: 
            \begin{align*}
            &- \theta\text{: decoder network}\\
            &- \phi\text{: encoder network}\\
            &- \psi\text{: logistic regression network}\\
            &- z\sim Z\text{: embeddings generated by }\phi(x)\\
            &- \bar{Z}\text{: rv for embeddings attached to prediction branch, }\bar{Z}\subset Z\\
            &- Y\text{: rv representing class labels}\\
            &- \beta,\eta,\gamma_{d},\gamma_{od}\in \mathbb{R} \text{ coefficients}
            \end{align*}

        """
        X, Y = data
        qz_x, decoded = self.forward(X)

        # Reconstruction
        recon = torch.square(decoded-X).sum(dim=1).mean()

        # KL-Divergence
        kl = dist.kl.kl_divergence(qz_x, self.pz).sum(-1).mean()

        # DIP-VAE I-II
        mu = qz_x.loc.clone()
        var = qz_x.scale.clone()**2
        varEz_x = torch.cov(mu.t())
        Evarz_x = torch.diag(var.mean(dim=0))
        cov_z = varEz_x + Evarz_x

        # diag
        dipvae_i = torch.square(torch.ones(
            self.latent_dim, device=self.device) - torch.diag(cov_z)).sum()
        # off-diag
        dipvae_ii = torch.square(cov_z.flatten()[1:].view(
            self.latent_dim-1, self.latent_dim+1)[:, :-1].flatten()).sum()

        dipvae = 10*dipvae_i + dipvae_ii

        # Prediction loss
        g = mu[:, -self.guided_dim:]
        guided_logits = self.guide(g)
        guided_preds = guided_logits.argmax(dim=1)
        guided = F.cross_entropy(guided_logits, Y.type(torch.int64))

        # total loss
        loss = recon + self.beta*kl + self.eta*guided + self.gamma*(dipvae)
        acc = (guided_preds == Y).sum().float()/(guided_preds.size(0))

        losses = ModelOutput(
            loss=loss,
            recon=recon,
            kl=kl,
            dipvae=dipvae,
            acc=acc
        )

        return losses

    def val_function(self, data):
        """
        Computes accuracy of prediction branch on unseen data

        Args:
            data (~torch.Tensor): validation data 

        Returns:
            output (ModelOutput): validation accuracy 
        """
        X, Y = data

        qz_x, _ = self.forward(X)
        mu = qz_x.loc.clone()

        g = mu[:, -self.guided_dim:]
        guided_logits = self.guide(g)
        guided_preds = guided_logits.argmax(dim=1)

        val_acc = (guided_preds == Y).sum().float()/guided_preds.size(0)

        return ModelOutput(val_acc=val_acc)

    def _elbo_scheduler_update(self, e):
        """
        adaptive updates of the loss terms over training.  Similar idea to capacitated training in
        https://arxiv.org/abs/1804.03599 
        """
        self.beta = self.model_config.beta*self.elbo_scheduler['beta'](e)
        self.eta = self.model_config.eta*self.elbo_scheduler['eta'](e)
        self.gamma = self.model_config.gamma*self.elbo_scheduler['gamma'](e)


class JointVAE(GuidedVAE):
    r"""
    Guided VAE with categorical latent variables

    Parameters:
        model_config (JointConfig): config file specifying generic properties of the vae 
        encoder (~base.Encoder): Instance of an encoder-like architecture.  Works with any :class:`~nn.Module` object  mapping input tensors to hyperparameters of the posterior over the embeddings. 
        decoder (~base.Decoder): Instance of a decoder-like architecture. Works with any :class:`~nn.Module` object mapping embeddings to reconstructions. 
        guide (~base.Guide): Logistic regression network to predict labels at latent scale 
    """

    def __init__(self, **kwargs):
        super().__init__(**kwargs)  # delegate

        self.disc_dim = self.model_config.disc_dim

        # discrete prior
        self.pc = dist.categorical.Categorical(
            probs=(1/self.disc_dim)*torch.ones(self.disc_dim, device=self.device))

    def forward(self, x):
        r"""Outputs posterior latent distributions and sample embeddings for batch 

        Args:
            x (~torch.Tensor): input batch 

        Returns:
            (qz_x, q_cx), decoded (~torch.distributions.normal.Normal, ~torch.distributions.normal.Normal), ~torch.Tensor: posterior distributions and reconstruction
        """
        enc = self.encoder(x)
        mu, log_var = enc['cont']
        alpha_logits = enc['disc']

        # sampling
        z_cont = mu + torch.exp(0.5*log_var) * \
            (torch.zeros(mu.shape, device=self.device).normal_())
        z_disc = self.sample_concrete(alpha_logits, tau=0.1)
        z = torch.cat((z_cont, z_disc), dim=1)

        decoded = self.decoder(z)

        # posteriors
        qz_x = dist.normal.Normal(mu, torch.exp(0.5*log_var))
        qc_x = dist.categorical.Categorical(logits=alpha_logits)

        return (qz_x, qc_x), decoded

    def sample_concrete(self, logits, tau=0.1):
        r"""
        Reparameterization trick for sampling categorical random variables allowing for backpropogation. 
        Relevant papers: https://arxiv.org/abs/1611.00712 and https://arxiv.org/abs/1611.01144. 

        Args:
            logits (~torch.Tensor): un-normalized log probs for catorical we're sampling from 
            tau (float): relaxation parameter, tau small is more similar to the actual categorial distribution 
        Returns:
            samples (~torch.Tensor): one-hot rv's sampled from the categorical distribution 
        """
        u = torch.zeros_like(logits, device=self.device).uniform_(0, 1)
        g = -torch.log(-torch.log(u))
        return F.softmax((logits+g)/tau, dim=1)

    def loss_function(self, data):
        r"""
        Multi-task objective for the case with discrete latents 

        .. math::
             \begin{align*}
            \mathcal{L}(\boldsymbol{\theta}, \boldsymbol{\phi}; x) &= \mathbb{E}_{z \sim q_{\boldsymbol{\phi}}(z|x)}\left[\log p_{\boldsymbol{\theta}}(x|z)\right] - 
                \beta \left( D_{KL}(q_{\boldsymbol{\phi}}(z|x)||p_{\boldsymbol{\theta}}(z)) + D_{KL}(q_{\boldsymbol{\phi}}(c|x)||p_{\boldsymbol{\theta}}(c))\right) \\
                & - \eta H(Y, \psi(\bar{Z})) - \left(\gamma_{od}\sum_{i\neq j}||[Cov(z)]_{i,j}|| + \gamma_{d}\sum_{i=1}^{d}||[Cov(z)]_{ii}-1||\right)
            \end{align*} 

        with 

        .. math :: 
            \begin{align*}
            &- \theta\text{: decoder network}\\
            &- \phi\text{: encoder network}\\
            &- \psi\text{: logistic regression network}\\
            &- z\sim Z\text{: continuous embeddings generated by }\phi(x)\\
            &- z\sim C\text{: discrete embeddings generated by }\phi(x)\\
            &- \bar{Z}\text{: rv for embeddings attached to prediction branch, }\bar{Z}\subset C\\
            &- Y\text{: rv representing class labels}\\
            &- \beta,\eta,\gamma_{d},\gamma_{od}\in \mathbb{R} \text{ coefficients}
            \end{align*}

        """
        X, Y = data

        # Model Feedforward
        (qz_x, qc_x), decoded = self.forward(X)

        # Reconstruction mse
        recon = torch.square(decoded-X).sum(dim=1).mean()

        # KL-Divergence [ decomposition ]
        kl_z = dist.kl.kl_divergence(qz_x, self.pz).sum(-1).mean()
        kl_c = dist.kl.kl_divergence(qc_x, self.pc).mean()
        kl = kl_z + kl_c

        mu = qz_x.loc.clone()
        var = qz_x.scale.clone()**2

        # DIP-VAE I-II
        varEz_x = torch.cov(mu.t())
        Evarz_x = torch.diag(var.mean(dim=0))
        cov_z = varEz_x + Evarz_x

        dipvae_i = torch.square(torch.ones(
            self.latent_dim, device=self.device) - torch.diag(cov_z)).sum()
        dipvae_ii = torch.square(cov_z.flatten()[1:].view(
            self.latent_dim-1, self.latent_dim+1)[:, :-1].flatten()).sum()
        dipvae = 10*dipvae_i + dipvae_ii

        # Guided
        g = self.sample_concrete(qc_x.logits, tau=0.1)
        guided_logits = self.guide(g)
        guided_preds = guided_logits.argmax(dim=1)
        guided = F.cross_entropy(guided_logits, Y.type(torch.int64))

        # Final loss
        loss = recon + self.beta*kl + self.eta*guided + self.gamma*(dipvae)
        acc = (guided_preds == Y).sum().float()/(guided_preds.size(0))

        losses = ModelOutput(
            loss=loss,
            recon=recon,
            kl=kl,
            dipvae=dipvae,
            acc=acc
        )

        return losses

    def val_function(self, data):
        X, Y = data

        (qz_x, qc_x), decoded = self.forward(X)

        g = self.sample_concrete(qc_x.logits, tau=0.1)
        guided_logits = self.guide(g)
        guided_preds = guided_logits.argmax(dim=1)

        val_acc = (guided_preds == Y).sum().float()/guided_preds.size(0)

        return ModelOutput(val_acc=val_acc)
