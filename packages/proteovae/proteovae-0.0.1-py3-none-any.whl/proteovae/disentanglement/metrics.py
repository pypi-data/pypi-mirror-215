import numpy as np
import torch
from tqdm import tqdm
from sklearn.cluster import AgglomerativeClustering
from itertools import combinations


def compute_disentanglement(zs, ys, L=100, M=20000):
    """Metric introduced in Kim and Mnih (2018)

    Args:
        zs (~torch.Tensor): embeddings generated by model encoder 
        ys (~torch.Tensor): corresponding labels 
        L (int): batch size 
        M (int): number of algo randomizations 

    Returns:
        ~torch.Tensor, ~torch.Tensor: raw and rowmax-normalized tableaus described in original method 
    """
    N, D = zs.size()
    _, K = ys.size()
    zs_std = torch.std(zs, dim=0)
    ys_uniq = [c.unique() for c in ys.split(1, dim=1)]  # global: move out
    V = torch.zeros(D, K, device=zs_std.device)
    # sample fixed-factor idxs ahead of time
    ks = np.random.randint(0, K, M)

    for m in range(M):
        k = ks[m]
        fk_vals = ys_uniq[k]

        # fix fk
        fk = fk_vals[np.random.choice(len(fk_vals))]

        # choose L random zs that have this fk at factor k
        zsh = zs[ys[:, k] == fk]
        zsh = zsh[torch.randperm(zsh.size(0))][:L]
        d_star = torch.argmin(torch.var(zsh / zs_std, dim=0))
        V[d_star, k] += 1

    return V, torch.max(V, dim=1)[0].sum() / M  # this is in contention


def tweak_importance_topk(model, zs, factor, topk=50, L=100, M=1000):
    """Metric for assessing how sensitive output features are to each latent dimension

    Args:
        model (~proteovae.models.models.BaseVAE): model whose decoder is used throughout the function call
        zs (~torch.Tensor): reference embeddings generated by the model 
        factor (int): which embedding to test the sensitivity of 
        topk (int): number of sensitve features to be considered (ranked desc)
        L (int): batch size
        M (int): number of randomizations

    Returns:
        ~torch.Tensor, ~torch.Tensor: raw sensitivity tableau, ordered indices desc for feature importance

    """
    dim_out = list(model.decoder.parameters())[-1].shape[0]
    V = torch.zeros(dim_out, 1, device=model.device)  # tableau

    for m in range(M):
        zsh = zs[np.random.permutation(zs.shape[0])][:L]

        # duplicate, sign change on guided latent, decode & diff
        hsz = np.copy(zsh)
        hsz[:, factor] = - zsh[:, factor]

        xs = model.decoder(torch.tensor(zsh, device=model.device))
        sx = model.decoder(torch.tensor(hsz, device=model.device))

        diff = torch.abs(xs-sx).mean(dim=0)
        idx = torch.topk(diff, k=topk)[1]

        V[idx] += 1

    return V.cpu().detach().numpy(), torch.topk(V, k=topk, dim=0)[1].cpu().detach().numpy().flatten()


def tweak_importance_massi(model, zs, factor, n_clusters=4, L=100, M=1000):
    """Clustering-based approach for tweak importance.  See ~proteovae.disentanglement.tweak_importance_topk


    Args:
        model (~proteovae.models.models.BaseVAE): model whose decoder is used throughout the function call
        zs (~torch.Tensor): reference embeddings generated by the model 
        factor (int): which embedding to test the sensitivity of 
        n_clusters (int): number of clusters for the sensitive features, only extreme cluster retained
        L (int): batch size
        M (int): number of randomizations

    Returns:
        ~torch.Tensor, ~torch.Tensor: raw sensitivity tableau, indices for each embedding dimension for sensitive features


    """
    dim_out = list(model.decoder.parameters())[-1].shape[0]
    V = np.zeros(dim_out)  # tableau

    for m in tqdm(range(M)):
        zsh = zs[np.random.permutation(zs.shape[0])][:L]

        # duplicate, sign change on guided latent, decode & diff
        hsz = np.copy(zsh)

        hsz[:, factor] = - zsh[:, factor]

        # hsz[:,factor] = np.random.normal(0,1, size = hsz.shape[0])

        xs = model.decoder(torch.tensor(zsh, device=model.device))
        sx = model.decoder(torch.tensor(hsz, device=model.device))
        diff = torch.abs(
            xs-sx).mean(dim=0).cpu().detach().unsqueeze(-1).numpy()

        # hierarchical clustering until 2 clusters for variable length sensitivity sets
        max_id = np.argmax(diff)

        hclust = AgglomerativeClustering(
            n_clusters=n_clusters, metric='euclidean', linkage='ward').fit(diff)
        y = np.zeros(dim_out)
        y[np.where(hclust.labels_ == hclust.labels_[max_id])[0]] = 1

        V += y

    # one last clustering
    max_id = np.argmax(V)
    hclust = AgglomerativeClustering(
        n_clusters=2, metric='euclidean', linkage='ward').fit(np.expand_dims(V, -1))

    y = np.zeros(V.shape[0])
    y[np.where(hclust.labels_ == hclust.labels_[max_id])[0]] = 1

    rel = {i: V[i] for i in np.where(y == 1)[0]}
    ordered_indices = [tup[0] for tup in sorted(
        rel.items(), key=lambda x: x[1], reverse=True)]

    return V, ordered_indices


def extract_partitions(model, zs, tweak_fn, d=10, L=100, M=1000):
    """Metric for determining how the feature space is partitioned amongst the latent units

    Args:
        model (~proteovae.models.models.BaseVAE): model whose decoder is used throughout the function call
        zs (~torch.Tensor): reference embeddings generated by the model 
        tweak_fn (function): wrapper for :meth:`tweak_importance_topk` or :meth:`tweak_importance_massi` 
    Returns:
        z_dict (dict), W (~torch.Tensor), set(int) : dictionary of sensitive features for each embedding dim, matrix of pairwise set intersections, unique sensitive featues for whole model


    .. code-block::

            >>> from proteovae.disentanglement import tweak_importance_topk, extract_partitions
            >>> from functools import partial
            ...
            >>> # ...
            >>> tweak_fn = partial(tweak_importance_topk, topk = 100)

            >>> z_dict, W, values = extract_partitions(model, 
            ...                                    zs = zs, 
            ...                                    tweak_fn=tweak_fn, 
            ...                                    d=model.latent_dim, 
            ...                                    L=300, 
            ...                                    M = 100)
            ...          
    """

    if not callable(tweak_fn):
        return

    z_dict = {}

    # step 1: populate the dict
    for i in tqdm(range(d)):
        V, idx = tweak_fn(model=model, zs=zs, factor=i, L=L, M=M)
        z_dict[i] = idx

    # step 2: intersect sensitive sets
    W = np.diag([len(v) for v in z_dict.values()])

    combs = list(combinations([i for i in range(d)], 2))

    for c in combs:
        size_ij = len(set(z_dict[c[0]]) & set(z_dict[c[1]]))
        W[c] = size_ij
        W[c[::-1]] = size_ij

    # step 3 compute the total number of unique proteins
    values = []
    for v in z_dict.values():
        values.extend(v)

    return z_dict, W, set(values)
